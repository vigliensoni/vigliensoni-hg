<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>vigliensoni</title><meta name=description content="Website for music artist and researcher Gabriel Vigliensoni."><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://vigliensoni.com/css/bootstrap.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic"><link rel=stylesheet href=https://vigliensoni.com/css/font-awesome.min.css><link rel=stylesheet href=https://vigliensoni.com/css/owl.carousel.css><link rel=stylesheet href=https://vigliensoni.com/css/owl.theme.css><link href=https://vigliensoni.com/css/style.default.css rel=stylesheet id=theme-stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link href=https://vigliensoni.com/css/custom.css rel=stylesheet><link rel="shortcut icon" href=https://vigliensoni.com/img/favicon.png></head><body><div id=all><div class=container-fluid><div class="row row-offcanvas row-offcanvas-left"><div id=sidebar class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas"><div class=sidebar-content><h1 class=sidebar-heading><a href=https://vigliensoni.com/>vigliensoni</a></h1><p class=sidebar-p>Music artist and researcher.</p><ul class=sidebar-menu><li><a href=https://vigliensoni.com/categories/featured>Featured</a></li><li><a href=https://vigliensoni.com/portfolio/>Portfolio</a></li><li><a href=https://vigliensoni.com/categories/music>Music</a></li><li><a href=https://vigliensoni.com/research/>Research</a></li><li><a href=https://vigliensoni.com/categories/presentation/>Workshops & talks</a></li><li><a href=https://vigliensoni.com/about/>About</a></li><li><a href=https://vigliensoni.com/contact/>Contact</a></li></ul><p class=social><a href=/contact/ data-animate-hover=pulse class=email title=E-mail><i class="fa fa-envelope"></i></a>
<a href=https://instagram.com/vigliensoni title class="external instagram" title=Instagram><i class="fa fa-instagram"></i></a>
<a href=https://github.com/vigliensoni data-animate-hover=pulse class=external title=GitHub><i class="fa fa-github"></i></a>
<a href=https://www.youtube.com/@vigliensoni/videos data-animate-hover=pulse class=external title=YouTube><i class="fa fa-youtube"></i></a>
<a href=https://facebook.com/vigliensoni data-animate-hover=pulse class="external facebook" title=Facebook><i class="fa fa-facebook"></i></a></p><div class=copyright><p class=credit>&copy;2024 Gabriel Vigliensoni<br><br>Template by <a href=https://bootstrapious.com/free-templates class=external>Bootstrapious.com</a>
& ported to Hugo by <a href=https://github.com/kishaningithub>Kishan B</a></p></div></div></div><div class="col-xs-12 col-sm-8 col-md-9 content-column white-background"><div class="small-navbar visible-xs"><button type=button data-toggle=offcanvas class="btn btn-ghost pull-left"> <i class="fa fa-align-left"></i>Menu</button><h1 class=small-navbar-heading><a href=https://vigliensoni.com/>vigliensoni</a></h1></div><div class=row><div class=col-lg-8><div class=content-column-content><h1>Steering latent audio models through interactive machine learning</h1><div class=tagbox><a id=date href=/years/2023>(2023)</a>&nbsp;
<a href=/tags/research>Research</a>&nbsp; <a href=/tags/paper>Paper</a></div><br><p>14th International Conference on Computational Creativity</p><p><a href=https://doi.org/10.5281/zenodo.8087978>Gabriel Vigliensoni and Rebecca Fiebrink. 2023. In <em>Proceedings of the 14th International Conference on Computational Creativity (ICCC 2023)</em>.</a></p><p>ABSTRACT</p><p>In this paper, we present a proof-of-concept mechanism for steering latent audio models through interactive machine learning. Our approach involves mapping the human-performance space to the high-dimensional, computer-generated latent space of a neural audio model by utilizing a regressive model learned from a set of demonstrative actions. By implementing this method in ideation, exploration, and sound and music performance we have observed its efficiency, flexibility, and immediacy of control over generative audio processes.</p><p>The video below shows our approach instantiated through a RAVE model trained on a large dataset of acoustic harmonic audio signals. The model has eight latent dimensions. A 2-dimensional human performance space is mapped to the RAVE (Caillon and Esling 2021) model through regressive model using FluCoMa (Tremblay et al. 2021)</p><div class="embed website"><iframe src=https://media.vigliensoni.com/video/iccc2023 width=100% height=400px frameborder=0 allowfullscreen allow="geolocation *; microphone *; camera *; midi *; encrypted-media *"></iframe></div><div class=text-caption>Supporting video demoing the proposed approach to steer latent audio models.</div><h3 id=references>References</h3><p>Caillon, A., and Esling, P. 2021. RAVE: A variational autoencoder for fast and high-quality neural audio synthesis. arXiv:2111.05011.</p><p>Tremblay, P. A.; Roma, G.; and Green, O. 2021. Enabling programmatic data mining as musicking: The Fluid Corpus Manipulation toolkit. Computer Music Journal 45(2):9â€“23.</p><br><br><br></div></div></div></div></div></div></div><script src=https://vigliensoni.com/js/jquery.min.js></script>
<script src=https://vigliensoni.com/js/bootstrap.min.js></script>
<script src=https://vigliensoni.com/js/jquery.cookie.js></script><script src=https://vigliensoni.com/js/ekko-lightbox.js></script>
<script src=https://vigliensoni.com/js/jquery.scrollTo.min.js></script>
<script src=https://vigliensoni.com/js/masonry.pkgd.min.js></script>
<script src=https://vigliensoni.com/js/imagesloaded.pkgd.min.js></script>
<script src=https://vigliensoni.com/js/owl.carousel.min.js></script>
<script src=https://vigliensoni.com/js/front.js></script></body></html>